---
title: "A Retrospective Bayesian Model for Measuring Covariate Effects on Observed COVID-19 Test and Case Counts"
author: 
  - Robert Kubinec:
      email: rmk7@nyu.edu
      institute: nyuad
      correspondence: true
  - Luiz Max Carvalho:
      institute: gvf
institute:
  - nyuad: New York University Abu Dhabi
  - gvf: School of Applied Mathematics, Getulio Vargas Foundation
date: "June 8th, 2020"
toc: false
output: 
  bookdown::pdf_document2:
    keep_tex: true
    includes:
      in_header:
          preamble.tex
    pandoc_args:
      - '--lua-filter=scholarly-metadata.lua'
      - '--lua-filter=author-info-blocks.lua'
  #bookdown::word_document2
bibliography: BibTexDatabase.bib
abstract: "As the COVID-19 outbreak progresses, increasing numbers of researchers are examining how an array of factors either hurt or help the spread of the disease. Unfortunately, the majority of available data, primarily confirmed cases of COVID-19, are widely known to be biased indicators of the spread of the disease. In this paper we present a retrospective Bayesian model that is much simpler than epidemiological models of disease progression but is still able to identify the effect of covariates on the historical infection rate. The model is validated by comparing our estimation of the count of infected to projections from expert surveys and extant disease forecasts. To apply the model, we show that as of May 25th, there are approximately 4 million infected people presently infected in the United States, and these people are increasingly concentrated in states with higher vote shares for President Donald Trump in 2016, higher rates of cardiovascular deaths, fewer medical providers and more smokers on average than other states. We also estimate the effect of state stay-at-home orders, showing that orders that were imposed long after COVID-19 infections were first discovered have proved to be more effective at stamping out the virus than lockdowns imposed very soon after positive cases were discovered. We use mediation analysis to show that the discrepancy is in part explained by the limited reductions in social contact as measured by Google mobility data for these social distancing measures, suggesting limited enforcement and compliance with the orders.^[To reproduce the model and to access the underlying Stan code, please see our [Github page](https://github.com/saudiwin/corona_tscs). This paper is part of the [CoronaNet project](https://lumesserschmidt.github.io/CoronaNet/) collecting data on government responses to the COVID-19 pandemic. For helpful comments we thank Cindy Cheng and Joan Barcelo.]"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning=FALSE,message=FALSE)
require(dplyr)
require(tidyr)
require(ggplot2)
require(cmdstanR)
require(stringr)
require(lubridate)
require(bayesplot)
require(historydata)
require(readr)
require(datasets)
require(extraDistr)
require(patchwork)
require(rstanarm) # for observed data modeling
require(tidycensus)
require(RcppRoll)
require(readxl)
require(ggrepel)
require(missRanger)
require(cmdstanr)
# update this package /w data

set.seed(662817)

knitr::opts_chunk$set(warning=F,message=F)

# NEED THESE GITHUB REPOS IN YOUR HOME FOLDER:

# https://github.com/COVID19Tracking/covid-tracking-data
# https://github.com/nytimes/covid-19-data

system2("git",args=c("-C ~/covid-tracking-data","pull"))
system2("git",args=c("-C ~/covid-19-data","pull"))

# whether to run model (it will take a few hours) or load saved model from disk

run_model <- F

# whether to use fresher coronanet policy data

new_policy <- F

# whether to pull new cases/tests from NYT/COVID project

new_cases <- F

```


\newpage



As more and more data has become available on observed case counts of the SARS-CoV2 coronavirus, there have been increasing attempts to infer how contextual factors like government policies, partisanship, and temperature affect the disease's spread [@carleton2020;@sajadi2020;@dudel2020;@tansim2020;@flaxman2020;@brze2020]. The temptation to make inferences from the observed data, however, can result in misleading conclusions. For example, some policy makers have publicly questioned whether the predictions of epidemiological models are far worse than the observed case count.^[See article available at https://www.realclearpolitics.com/video/2020/03/26/dr_birx_coronavirus_data_d] By contrast, in this paper we show that the unobserved infection rate is a confounding variable affecting any estimates of covariates on the observed counts of COVID-19 cases and tests. For this reason, in this paper we present a retrospective Bayesian model that can adjust for this bias by estimating the unseen infection rate up to an unidentified constant. Furthermore, by incorporating informative priors based on serological surveys of infection prevalence, it is possible to put an informative prior on the unobserved infection rate and estimate both recent disease trends and the effect of covariates on the historical spread of the disease.

We also show how the model can be applied by measuring the association between U.S. state-level factors and the disease as of July 10, 2020, including the the duration and timing of stay-at-home orders, Google mobility data and protest activity. We find that, when controlling for stay-at-home orders, increasing vote share for Donald Trump in the 2016 elections by 10 percentage points is associated with an increase of the marginal infection rate of 0.004% (HPD ). In addition, the number of cardiovascular deaths and air particle PM$_2.5$ concentrations positively predict infection rates, while higher rates of smoking, number of health providers, youth and foreign-born residents are negatively associated with infection rates. We find that protests for racial justice had a small positive effect on infection rates.

To examine the effect of lockdowns and social-distancing mesaures, we further implement mediation analysis to enable us to see the direct effect of lockdowns on the infection rate versus the indirect of lockdowns via Google mobility data on the infection rate. We include the number of days after the first reported in-state COVID-19 case as an additional moderator. Our analysis shows that the controlled direct effect of lockdowns is heavily conditioned by how long the lockdown was imposed following the first COVID-19 case, with later lockdowns far less effective than earlier lockdowns.

The mediation analysis further shows that the effect of lockdowns through decreasing retail and workplace traffic is strongly depressing of infection rates. Furthemore, the indirect effects are not moderated by the date the lockdown was imposed, suggesting that later lockdowns have been less effective because they have been less able to control mobility. Paradoxically, the effect of lockdowns through transit mobility has been positive, though it is not clear why this may be the case.

# Methods

In this section we present a formal definition of the model. We refer the reader to the supplementary materials for details of Monte Carlo simulations showing recovery of the latent infection rate. 

Compartmental models employed by epidemiologists to study disease, and in particular SARS-CoV2 [@peak2020;@riou2020;@verity2020;@perkins2020;@lourenco2020;@li2020;@ferguson2020], suppose different classes (compartments) of individuals in the population, denoted $S$ for susceptible, $I$ for infectious, and $R$ for removed (other compartments may be added such as $E$ for exposed). The model is usually written in the form of a system of ordinary differential equations (ODEs) and assumes a fixed population size, as seems reasonable during a relatively quick epidemic.
The number infected individuals can then be obtained from the solution of the ODE system for the $I$ compartment.
These models guide our understanding of the disease and its progression, and have made warnings about the disease's spread that are proving true on a daily basis.

By contrast, this paper endeavors to estimate a much simpler quantity than the entire evolution of the outbreak. Many researchers and the general public often want to learn about what has already happened, or the *empirical* infection rate (also called the attack rate in the epidemiological literature). For a number of time points $t \in T$ since the outbreak's start and countries/regions $c \in C$, we aim to identify the following quantity:

$$
f_t \left (\frac{I_{ct}}{S_{ct}+R_{ct}} \right )
$$

Assuming a fixed population size, this quantity is simply the marginal rate of infections in the population up to the present.
The function $f_t$ determines the historical time trend of the rate of infection (which is assumed to be same across countries/regions) in the population up to time $T$, the present. Because the denominator is shifting over time due to disease progression dynamics, this model is only useful for retrospection, i.e., to examine factors that may be influencing the empirical time trend $f_t$. As $S_{ct}$ and $R_{ct}$ are exogenous to the model, the model cannot predict future prevalence of the disease given that it does not determine these crucial factors. In other words, this model can be seen as a local linear approximation to the $I_{ct}$ curve from an SIR model.

However, we do not have estimates of the actual infected rate $I_{ct}$, only positive COVID-19 cases $a_{ct}$ and numbers of COVID-19 tests $q_{ct}$. Given this limitation, the aim of the model is to backwards infer the infection rate $I_{ct}$ as a latent process given observed test and counts. Modeling the latent process is necessary to avoid bias in using only observed case counts as a proxy for $I_{ct}$. The reason for this is shown in Figure \ref{tikzfig} in which a covariate $X_{ct}$, such as temperature, is hypothesized to affect the infection rate $I_{ct}$. Unfortunately, increasing infection rates can cause both increasing numbers of observed counts $a_{ct}$ and tests $q_{ct}$. As more people are infected, more tests are likely to be done, which will increase the number of cases independently of the infection rate. As a result, due to the back-door path from the infection rate $I_{ct}$ to case counts $a_{ct}$ via the number of tests $q_{ct}$, it is impossible to infer the effect of $X_{ct}$ on $I_{ct}$ from the observed data alone without modeling the latent infection rate. 

\begin{figure}
\label{tikzfig}
\caption{Directed Acyclic Graph Showing Confounding of Covariate $X_{ct}$ on Observed Tests $q_{ct}$ and Cases $a_{ct}$ Due to Unobserved Infection Rate $I_{ct}$}
\ctikzfig{policy_dag}
\footnotesize{Figure shows the relationship between a covariate $X_{ct}$ representing a policy or social factor influencing the infection rate $I_{ct}$. Because the infection rate $I_{ct}$ influences both the number of reported tests $q_{ct}$ and reported cases $a_{ct}$, any regression of a covariate $X_{ct}$ on the reported data will be biased.}
\end{figure}

Given this intuitive overview, we turn to a more formal definition. The outcome to be explained is more specifically defined as the tests and cases reported on a given day rather than the cumulative total for ease of modeling purposes (i.e., the lagged difference of the cumulative total). We assume that $I_{ct}$ is an unobserved Beta-distributed variable with a 3-order polynomial time trend of the number of post-outbreak time periods $T_O < T$, where an outbreak begins at the first reported case in a given area. The third-order polynomial reflects the fact that epidemics occur in waves, although the curve is unlikely to be symmetric as a simpler quadratic function would require.
By using a single time function, the model assumes that the coronavirus follows a similar pattern of infection across states, as appears to be the case (i.e., all states are infected with the same virus).

The unobserved infection rate $I_{ct}$ is a function of the following parameters:

\begin{align}
\operatorname{Pr}(I_{ct} \mid T=t) \sim \operatorname{Beta}(&\alpha_1 + \beta_{O1}\sum_{c=1}^{C} \textbf{1}(a_{ct'}>0) \forall t'<t + \beta_{S1}X_{ct} + \\
                        &\beta_{I1}t_o + \beta_{I2}t_o^2 + \beta_{I3}t_o^3, \phi),
  (\#eq:binom)
\end{align}

where $g(\cdot)$ is the inverse logit function, $\beta_{O1}\sum_{c=1}^{C} \textbf{1}(a_{ct-1}>0) \forall t'<t$ is the sum of states with at least one case of infection in the world at any previous time point, which we term the world infection rate, and the three $\beta_{Ii}$ are polynomial coefficients of the number of post-outbreak time periods $t_o$.

The world infection parameter measures the spread of the virus due to travel across state borders, and as such will increase rapidly as more states are infected.
By contrast, the polynomial time trends represent possible within-state transmission of the virus, which can cause exponentially growing case counts compared to the linear sum of infected states parameter.
Finally, the parameter $\phi$ is a dispersion parameter governing how precise the infection rate estimate is. 

Given these two primary ways that the infection rate can increase, we can model the effect of background covariates and suppression policies as $\beta_{S1}X_{ct}$. We can include linear time counters in $X_c$ and interactions to more flexibly model time-varying covariates as well.

<!-- The second way suppression measures enter the model is through $\beta_{S2}X_c$, which can increase over time as the virus increases. This parameter reflects possible measures which will grow more effective as domestic transmission of the virus increases (i.e. as the polynomial time trend takes off). -->
<!-- As such, it is assumed that any deviation from the common domestic transmission pattern is due to these time-varying suppression measures. -->

Once an outbreak has started in state $c$, which is indicated by a positive case count in time $t-1$, a time counter $t_o=1$ starts for that state and increases by 1 for each following time point until $T$.

Given this specification of the infection rate process, we can then move to the generation of the observed data, tests $q_{ct}$ and cases $a_{ct}$.
The infection rate is assumed to influence both of these quantities.
First, an increasing number of infections is associated with more tests as states try to identify who may have the virus. Furthermore, a rising infection rate is associated with a higher ratio of positive results (reported cases) conditional on the number of tests.
We model both of these observed indicators, tests and cases, jointly to simultaneously adjust for the infection rate's influence on both factors.

To model the number of tests, we assume that each state has an unobserved level of testing parameter, $\beta_{cq}>0$, indicating how strongly each state is willing and able to perform tests as a factor of the unobserved infection rate.
The number of observed tests $q_{ct}$ for a given time point $t$ and state $c$ conditional on  the states' population, $c_{p}$, has a binomial distribution: 

\begin{equation}
q_{ct} \sim \operatorname{Binomial}(c_{p}, g(\alpha_2 + \beta_{cq}I_{ct})).
(\#eq:binom2)
\end{equation}

The parameter $\beta_{cq}$ serves to scale the infection rate $I_{ct}$ so that an increasing infection rate has heterogeneous effects on the number of tests by state.
The intercept $\alpha_2$ indicates how many tests would be performed in a state with an infection rate of zero.
It is assumed that this number is quite low, though not necessarily zero. 

Given the parameter $\beta_{cq}$, a state could test almost no one or test far more than are actually infected depending on their willingness to impose tests.
However, the number of tests is increasing in $I_{ct}$ conditional on a state's willingness to test people.
That is, regardless of how much a state wants to test people, as the outbreak grows the number of tests will increase though at very different rates.^[For a very compelling visualiation of this process with empirical data from the COVID-19 pandemic, we refer the reader to this website: https://ourworldindata.org/grapher/covid-19-tests-cases-scatter-with-comparisons.]

Given the number of observed tests $q_{ct}$, we can then generate the number of observed cases $a_{ct}$ as a binomial random variable conditional on the number of tests $q_{ct}$:

\begin{equation}
a_{ct} \sim \operatorname{Binomial}(q_{ct}, g(\alpha_3 + \beta_a I_{ct})),
(\#eq:binom3)
\end{equation}
where $g(\cdot)$ is again the inverse logit function, $\alpha_3$ is an intercept that indicates how many cases would test positive with an infection rate of zero (equal to the false positive rate of the test), and $\beta_a>0$ is a parameter that determines how hard it is to find the infected people and test them as opposed to people who are not actually infected.
The multiplication of this parameter and the infection rate determines the observed number of cases $a_{ct}$ as a proportion of the number of observed tests $q_{ct}$. 

To summarize the model, infection rates determine how many tests a state is likely to undertake and also the number of positive tests they receive conditional on a certain number of tests.
This simultaneous adjustment helps takes care of mis-interpreting the observed data by not taking into account varying testing rates, which is likely why some policy makers argue that the epidemiological models are wrong.

Because sampling from a model with a hierarchical Beta parameter can be difficult, we can simplify the final likelihood by combining the beta distribution and the binomial counts into a beta-binomial model for tests:

\begin{equation}
q_{ct} \sim \operatorname{Beta-Binomial}(c_p, g(\alpha_2 + \beta_qI_{ct}), \phi_q),
(\#eq:binom4)
\end{equation}

and cases:

\begin{equation}
a_{ct} \sim \operatorname{Beta-Binomial}(q_{ct}, g(\alpha_3 + \beta_a I_{ct}), \phi_a).
(\#eq:binom5)
\end{equation}

For computational reasons, the infection rates $I_{ct}$ are put in to the beta-binomial model on the logit scale instead of transforming them to (0,1), but they can be transformed to proportions post-estimation with the inverse logit function.

## Identification

This model contains an unobserved latent process $I_{ct}$, and as such there are further constraints necessary in order to have a unique scale and rotation of the latent variable.
The most important restrictions are positivity constraints on the parameters $\beta_a$ and $\beta_q$ that govern the relationship between the infection rate and test and case counts.
It is assumed that infection rates will increase both the number of tests and the number of cases relative to the number of tests, though at necessarily different rates.

To add further indentification, it would be possible to fix or put an informative prior on $\alpha_3$ as the intercept necessarily equals the false positive rate of a COVID-19 test.
The actual false positive rate of the tests being used for the virus is very low, near zero for the PCR tests, according to the Federal Drug Administration.^[See https://www.fda.gov/media/136151/download.] For the purposes of this simulation, we will allow $\alpha_3$ to float as it will not affect identification of the sign of the covariates.

As we will show, no other identification restrictions are necessary to estimate the model beyond weakly informative priors assigned to parameters.
These are:
\begin{align}
\beta_a &\sim \operatorname{Exponential}(.1),\\
\beta_{qc} &\sim \operatorname{Exponential}(\sigma_q),\\
\sigma_q &\sim \operatorname{Exponential}(.1),\\
\beta_{Si} &\sim \operatorname{Normal}(0, 2),\\
\beta_{Ii} &\sim \operatorname{Normal}(0, 5),\\
\alpha_1 &\sim \operatorname{Normal} (0,10),\\
\alpha_2 &\sim \operatorname{Normal}(0,10),
(\#eq:binom6)
\end{align}
where the normal distribution is parametrized in terms of mean and standard deviation.

The one prior to note is that a hierarchical regularizing prior is put on the varying testing adjustment parameters $\beta_{qc}$ for regularization purposes due to the limited data available to inform the parameter.

Other than these weakly informative priors, the model is identified, as we show in the next section.
However, it is important to emphasize that there is no information in the model that identifies the *true* number of infected people. Rather, the infection rate is a latent process, and as such it is not known exactly what scale to assign to it without further information. However, both the relative growth in infection rates are identified, along with the effect of suppression measures, so *the model is useful without being fully identified*.
Furthermore, by incorporating insights from SIR/SEIR models we can also identify the latent scale with reasonable informative priors, as we show in the data analysis section.

We note that an advantage of this framework is providing a way to measured the count of infected adjusting for known biases in the number of tests. By comparing numbers of tests per capita and growth rates in cases across regions, the model is able to backwards infer a likely number of infected individuals in a given area. As such it exploits both within-area and between-area variance to adjust for the biases of imperfect testing.

We can extend this model further by considering the mediation of covariates $X_{ct}$ by adding a mediator $M_{ct}$ to the causal diagram, as in Figure \@ref(fig:tikzfig2). Because the mediator operates on the latent variable, it can allow us to separate the direct from indirect effects of the covariate on the infection rate. Adding this step into the model only requires an additional likelihood for the mediator-covariate relationship as in @Yuan2009. With the joint posterior estimates, it is straightforward to decompose the effect of $X_{ct}$ into indirect and direct effects employing the total derivative of the infection rate from @winship1983. 

\begin{figure}
\label{tikzfig2}
\caption{Directed Acyclic Graph for Latent Infection Rate with Mediator}
\ctikzfig{policy_dag_mediate}
\footnotesize{This figure adds a mediator $M_{ct}$ that mediates the relationship between a covariate $X_{ct}$ and the latent infection rate $I_{ct}$.}
\end{figure}

To add continuous mediation covariates $M_{ct}$, we add the following likelihood to the joint posterior:

\begin{equation}
M_{ct} \sim N(\alpha_m +_ \gamma X_{ct},\sigma_m)
(\#eq:mediate)
\end{equation}

as well as include $M_{ct}$ as a linear predictor in \@ref(eq:binom).

# Results

The only data required to fit the model, in addition to the covariates of interest, are observed cases and tests for COVID-19 by day. In this section, we fit the model to numbers of COVID-19 case counts on US states and territories provided by [The New York Times](https://github.com/nytimes/covid-19-data). By doing so, we can use the differences in trajectories across states to help identify the effect of state-level covariates on the infection rate. We supplement these observed case counts with testing data by day from the [COVID-19 Tracking Project](https://github.com/COVID19Tracking/covid-tracking-data). The testing data starts at March 4th, so we impute the testing data back in time by assuming that the average case/tests ratio stays the same to the origin of the outbreak. Furthermore, as there are discrepancies where the reported number of tests in some states like New Jersey is less than the total number of cases, we impute the number of tests via the case/test ratio for the sample as a whole.

To analyze the effect of suppression policies, we use data on counts of social distancing policies, restrictions on mass gatherings, restrictions on businesses, restrictions on government services, and stay-at-home orders from the CoronaNet Government Response dataset [@cheng2020]. For each type of policy, we include a variable representing the count of policies in that category effective for a particular day for each state. We separately include counts of policies concerning face masks, which also varies over time, though we do not consider these policies to be suppression policies as such.

To better understand over-time factors that may also affect COVID-19, we include polling data from Civiqs and Yougov at the state level. From Civiqs we include state-level polling averages by day for the percentage of respondents favoring Trump, percentage reporting the economy is "very good", and the percentage reporting that they are "extremely concerned" about the coronavirus. From YouTube we use a poll from May 8th reporting average number of respondents who said they used masks by U.S. state. As this poll does not vary over time, we include it as a predictor for the entire time series. 

To better understand the mediating effects of suppression policies, we include Google mobility data^[See https://www.google.com/covid19/mobility/] for retail, residential, parks, workplaces, transit and retail establishments. These estimates are by day and aggregated to the state level. They are measured in terms of an index that is initialized with a value of 100 at the index start on February 15th, 2020. To test for mediation, we include these as predictors of the infection rate, and separately fit a linear model (Normal distribution) with each mobility covariate as an outcome and the suppression covariates as predictors. 

All time-varying covariates--polling, protests and mobility data--are lagged by 14 days to account for the likely delay in events showing up in reported cases.

We further add in non-varying state-level data on Donald Trump's vote share for the 2016 election from the MIT Election Lab, a 2019 estimate of state GDP from the Bureau of Economic Analysis, the 2018 percentage of foreign born residents from the U.S. Census Bureau, and 2019 state-level average data on air pollution,^[Defined as average exposure of the general public to particulate matter of 2.5 microns or less (PM$_{2.5}$) measured in micrograms per cubic meter (3-year estimate).] cardiovascular deaths per capita, percentage of residents under age 18, number of dedicated health care providers, public health funding, and smoking rates provided by the United Health Foundation [@unhf2019]. All variables are standardized to permit comparability. 

In this section we fit the model with informative prior information derived from CDC serological surveys from April and May on the likely number of infected people in several U.S. states so that we can translate the estimates into probable infection rates. This serological data is shown in Table \@ref(tab:cdc).

Table: (\#tab:cdc) Geographic Serological Surveys from the Centers for Disease Control

 Date of Survey    Location                    Sample Size   Infection Prevalence (%)
----------------  --------------------------- ------------- --------------------------
  April 1, 2020     Western Washington State   3,265            1.13
  April 1, 2020     New York City              2,482            6.93
 April 10, 2020     South Florida              1,742            1.85
 April 26, 2020     Missouri                   1,882            2.65
 May 3, 2020        Utah                       1,132            2.18
 May 3, 2020        Connecticut                1,431            4.94




```{r munge_data,include=F}

# vote share
# MIT Election Lab
load("../data/mit_1976-2016-president.rdata")

vote_share <- filter(x,candidate=="Trump, Donald J.",
                     party=="republican",
                     writein=="FALSE") %>% 
  mutate(trump=candidatevotes/totalvotes)

# state GDP

state_gdp <- readxl::read_xlsx("../data/qgdpstate0120_0_bea.xlsx",sheet="Table 3") %>% 
  mutate(gdp=Q1 + Q2 + Q3 +Q4)

# state-level unemployment (week-varying)
# too old to be of much use
unemp <- read_csv("../data/simulation/unemployment/unemployment.csv")

# US Census data - population & percent foreign-born
# note: you need a Census API key loaded to use this -- see package tidycensus docs and use 
# function census_api_key with the key number and install=T

census_api_key(Sys.getenv("CENSUS_API_KEY"))

acs_data <- get_acs("state",variables=c("B01003_001","B05002_013"),year=2018,survey="acs1") %>% 
  select(-moe) %>% 
  mutate(variable=recode(variable,
                         B01003_001="state_pop",
                         B05002_013="foreign_born")) %>% 
  spread("variable","estimate") %>% 
  mutate(prop_foreign=foreign_born/state_pop)


# health data

health <- read_csv("../data/2019-Annual.csv") %>% 
  filter(`Measure Name` %in% c("Air Pollution","Cardiovascular Deaths","Dedicated Health Care Provider",
                              "Population under 18 years", "Public Health Funding","Smoking")) %>% 
  select(`Measure Name`,state="State Name",Value) %>% 
  distinct %>% 
  spread(key="Measure Name",value="Value")

merge_names <- tibble(state.abb,
                      state=state.name)

# google mobility data

goog_mobile <- read_csv("https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv?cachebust=b8b0c30cbee5f341",
                        col_types = cols(sub_region_2=col_character())) %>% filter(sub_region_1 %in% merge_names$state,
                                                   is.na(sub_region_2),
                                                   country_region=="United States") %>% 
  rename(state=sub_region_1,
         retail="retail_and_recreation_percent_change_from_baseline",
         grocery="grocery_and_pharmacy_percent_change_from_baseline",
         parks="parks_percent_change_from_baseline",
         transit="transit_stations_percent_change_from_baseline",
         workplaces="workplaces_percent_change_from_baseline",
         residential="residential_percent_change_from_baseline")

# impute some of this data with random forests / some missingness in parks and retail

goog_mobile <- missRanger(goog_mobile,pmm.k=5L)

if(new_cases) {
  
  nyt_data <- read_csv("~/covid-19-data/us-states.csv") %>% 
  complete(date,state,fill=list(cases=0,deaths=0,fips=0)) %>% 
  mutate(month_day=ymd(date)) %>% 
  group_by(state) %>% 
    arrange(state,date) %>% 
  mutate(cases=floor(c(cases[1:6],roll_mean(cases,n=7))),
         Difference=coalesce(cases,0)) %>% 
  left_join(merge_names,by="state")
  
  saveRDS(nyt_data,"nyt_data.rds")

tests <- read_csv("~/covid-tracking-data/data/states_daily_4pm_et.csv") %>% 
  mutate(month_day=ymd(date)) %>% 
  arrange(state,month_day) %>% 
  group_by(state) %>% 
  # first do 3-day moving average
  mutate(total=floor(c(total[1:6],roll_mean(total,n=7)))) %>% 
  mutate(tests_diff=total) %>% 
  select(month_day,tests="tests_diff",total,state.abb="state",recovered)

  saveRDS(tests,"tests.rds")

} else {
  
  nyt_data <- readRDS("nyt_data.rds")
  tests <- readRDS("tests.rds")
  
}

# recode bad testing information




# merge cases and tests

combined <- left_join(nyt_data,tests,by=c("state.abb","month_day")) %>% 
  left_join(acs_data,by=c("state"="NAME")) %>% 
  filter(!is.na(state_pop))

# add protest data
# need to impute
prot_data <- read_csv("../data/simulation/protest/Protest_Data_Final_Merged.csv") %>% 
  mutate(state=recode(state,
                      NewYork="New York",
                      SouthDakota="South Dakota",
                      NewJersey="New Jersey",
                      Connecticuticut="Connecticut",
                      DistrictofColumbia="District of Columbia",
                      NewHampshire="New Hampshire",
                      NewMexico="New Mexico",
                      NorthCarolina="North Carolina",
                      NorthDakota="North Dakota",
                      PuertoRico="Puerto Rico",
                      RhodeIsland="Rhode Island",
                      SouthCarolina="South Carolina",
                      SouthDakota="South Dakota",
                      WestVirginia="West Virginia")) %>% 
  missRanger(pmm.k=10L) %>% 
  group_by(state,date) %>% 
  summarize(sum_prot=sum(avgsize))

# protests by day

# group_by(prot_data,state,Date) %>% 
#   summarize(n_prot=sum(Attendees)) %>% 
#   ggplot(aes(y=n_prot,x=Date)) +
#   geom_line(aes(group=state))

# add polling data 

source("luca_scraping_code.R")

approval <- read_csv("../data/simulation/Civiqs/approve_president_trump.csv") %>%   select(date,state="state_col",trendline_approve)
concern <- read_csv("../data/simulation/Civiqs/coronavirus_concern.csv") %>% 
  select(state="state_col",date,trendline_extremely_concerned)
economy <- read_csv("../data/simulation/Civiqs/economy_family_retro.csv") %>% 
  select(date,state="state_col",trendline_gotten_worse)
local_gov_response <- read_csv("../data/simulation/Civiqs/coronavirus_response_local.csv") %>% 
  select(date,state="state_col",trendline_not_very_satisfied)

masks <- read_csv("../data/simulation/masks/masks_yougov.csv") %>% 
  select(state="X.1",
         mask_wear="% that selected")

#min_mask <- min(masks$mask_wear)

# add suppression data

if(new_policy) {
  coronanet <- read_csv("../data/CoronaNet/coronanet_release.csv") %>% 
  filter(country=="United States of America",
         type %in% c("Health Testing","Lockdown","Quarantine","Restriction and Regulation of Government Services",
                     "Restriction and Regulation of Businesses",
                     "Restrictions of Mass Gatherings",
                     "Social Distancing","Health Resources")) %>% 
  filter(type_sub_cat=="Masks" || type!="Health Resources")

# need to replace end date

coronanet <- group_by(coronanet,country,province,policy_id) %>% 
  mutate(date_end=case_when(update_type=="End of Policy" & !is.na(date_end)~date_end,
                                                  update_type=="End of Policy" & is.na(date_end)~date_start,
                                        any(!is.na(date_end[date_start==max(date_start,na.rm=T)]))~unique(max(date_end,na.rm=T)),
                                        TRUE~lubridate::today())) %>% 
  ungroup %>% 
  mutate(type_sub_cat=coalesce(type_sub_cat,"General"),
         type=recode(type,Lockdown="Quarantine"))

  write_csv(coronanet,"coronanet_data.csv")
} else {
  coronanet <- read_csv("coronanet_data.csv")
}



# do a summing exercise over policy types in terms of what is still available at a given day
count_pol <- parallel::mclapply(unique(coronanet$province), function(p) {
    
    # loop over policies
      these_pol <- unique(coronanet$policy_id[coronanet$province==p])
    
    lapply(these_pol, function(t) {
      
      if(!is.na(p)) { 
        this_chunk <- filter(coronanet,policy_id==t,province==p)
      } else {
        this_chunk <- filter(coronanet,policy_id==t)
      }
      
      
      # loop over days 
      
      lapply(seq(ymd("2019-12-30"),today(),by=1), function(d) {
        
        this_day_pol <- table(this_chunk$type_sub_cat[d>this_chunk$date_start & d<this_chunk$date_end])
        
        if(length(this_day_pol)==0) {
          tibble(month_day=d,
                 type=paste0(unique(this_chunk$type),collapse=";"),
                 type_sub_cat=unique(this_chunk$type_sub_cat),
                 count_pol_eff=rep(0,length(unique(this_chunk$type_sub_cat))),
                 province=p)
        } else {
          
          tibble(month_day=d,
                 type=paste0(unique(this_chunk$type),collapse=";"),
                 type_sub_cat=names(this_day_pol),
                 count_pol_eff=as.numeric(this_day_pol),
                 province=p)
        }
        
      }) %>% bind_rows
      
    }) %>% bind_rows
  
},mc.cores=16) %>% bind_rows

# sum over multiple overlapping policies

count_pol_sum <- mutate(count_pol,
                    type=recode(type,`Restriction and Regulation of Businesses;Restrictions of Mass Gatherings`="Restriction and Regulation of Businesses")) %>% 
                    group_by(month_day,type,province) %>% 
  summarize(sum_pol=sum(count_pol_eff))

count_pol_sum <- spread(count_pol_sum,key="type",value="sum_pol") %>% 
  mutate_at(vars(`Health Resources`:`Social Distancing`),~ifelse(month_day==min(month_day),
                                                               coalesce(.,0),
                                                               .)) %>% 
  fill(`Health Resources`:`Social Distancing`,.direction=c("down"))

combined <- left_join(combined, count_pol_sum,by=c("state"="province","month_day"))

# add in civiqs

combined <- left_join(combined,approval,by=c("state","month_day"="date")) %>% 
  left_join(concern,by=c("state","month_day"="date")) %>% 
  left_join(economy,by=c("state","month_day"="date")) %>% 
  left_join(local_gov_response,by=c("state","month_day"="date"))

# add in other datasets 

combined <- left_join(combined,health,by="state")
combined <- left_join(combined,select(state_gdp,state,gdp),by="state")
combined <- left_join(combined,select(vote_share,state,trump))
combined <- left_join(combined,select(goog_mobile,state,month_day="date",retail:residential))
combined <- left_join(combined,select(prot_data,state,date,sum_prot),by=c(month_day="date",
                                                                           "state")) %>% 
  mutate(sum_prot=sum_prot/state_pop,
         sum_prot=coalesce(sum_prot,0))
combined <- left_join(combined,select(masks,state,mask_wear),by="state")

# impute data

combined <- group_by(combined,state) %>% 
  mutate(test_case_ratio=sum(tests,na.rm=T)/sum(Difference,na.rm=T)) %>% 
  ungroup %>% 
  mutate(test_case_ratio=ifelse(test_case_ratio<1 | is.na(test_case_ratio),
                                mean(test_case_ratio[test_case_ratio>1],na.rm=T),test_case_ratio)) %>% 
  group_by(state) %>% 
    mutate(tests=case_when(Difference>0 & is.na(tests)~Difference*test_case_ratio,
                    Difference==0~0,
                    Difference>tests~Difference*test_case_ratio,
                    Difference==tests~Difference*test_case_ratio,
                    TRUE~tests),
           gdp=gdp/state_pop,
           Difference=ifelse(Difference<0,0,Difference)) %>% 
  filter(state!="Puerto Rico")

combined <- group_by(combined,state) %>% 
  arrange(month_day) %>% 
  mutate(outbreak=as.numeric(cases>1),
         lin_counter=1:n()) %>% 
  fill(outbreak,.direction="down") %>% 
  mutate(outbreak_time=cumsum(outbreak)) %>% 
  ungroup %>%
  group_by(state) %>% 
  arrange(state,month_day) %>% 
  mutate(world_infect=Difference - coalesce(dplyr::lag(Difference),0)) %>% 
  group_by(month_day) %>% 
  mutate(world_infect=sum(world_infect)) %>% 
  group_by(state) %>% 
  arrange(state,month_day) %>% 
  mutate(cases_per_cap=Difference/(state_pop),
         cases_per_cap=ifelse(cases_per_cap==0,.00000001,cases_per_cap)) %>% 
  mutate_at(c("grocery",
              "parks",
              "residential",
              "retail",
              "transit",
              "workplaces",
              "Health Resources",
              "Health Testing",
              "Restriction and Regulation of Businesses",
              "Restrictions of Mass Gatherings",
              "Quarantine",
              "Restriction and Regulation of Government Services",
              "Social Distancing",
              "sum_prot",
              "trendline_approve",
              "world_infect",
              "trendline_gotten_worse",
              "trendline_extremely_concerned",
              "trendline_not_very_satisfied"), ~dplyr::lag(.,n=14)) %>% 
    ungroup %>% 
  mutate_at(c("grocery",
              "parks",
              "residential",
              "retail",
              "transit",
              "workplaces",
              "trump",
              "gdp",
              "Dedicated Health Care Provider",
              "Smoking",
              "Population under 18 years",
              "Air Pollution",
              "sum_prot",
              "Cardiovascular Deaths",
              "Public Health Funding",
              "trendline_approve",
              "trendline_gotten_worse",
              "trendline_extremely_concerned",
              "trendline_not_very_satisfied",
              "Health Resources",
              "Health Testing",
              "mask_wear",
              "Restriction and Regulation of Businesses",
              "Restrictions of Mass Gatherings",
              "Quarantine",
              "Restriction and Regulation of Government Services",
              "Social Distancing"), ~as.numeric(scale(.))) %>% 
  mutate_at(c("lin_counter"),~scale(.,scale=F)) %>% 
  filter(!is.na(grocery),!is.na(trendline_extremely_concerned),!is.na(trendline_gotten_worse),!(Difference==0 & tests==0)) %>% 
  group_by(state) %>% 
  arrange(state,month_day) %>% 
  mutate(test_max=coalesce(tests - dplyr::lag(tests),tests),
         test_max=c(test_max[1:6],roll_mean(test_max,n=7)),
         test_max=cummax(test_max),
         test_max=as.numeric(scale(test_max/state_pop)))

min_mask <- min(combined$mask_wear)

combined <- combined %>% 
  group_by(state) %>% 
  mutate(mask_wear=ifelse(ymd("2020-04-03")<month_day,mask_wear,min_mask/2)) %>% 
  filter(state %in% sample(state,20))

# need to impute negative test numbers

combined <- group_by(combined,state) %>% 
  arrange(state,month_day) %>% 
  mutate(tests2=ifelse(tests < cummax(coalesce(tests,0)),NA,tests),
         tests3=imputeTS::na_interpolation(tests2,option="linear"))

# record original number of rows

rows_num_orig <- nrow(combined)

# need to calculate world infection parameter (use lag to adjust)

world_infect <- select(ungroup(combined),world_infect,month_day) %>% distinct 

world_infect <- arrange(world_infect,month_day) %>% 
  mutate(world_infect=as.numeric(scale(world_infect)))

combined <- left_join(select(combined,-world_infect),world_infect,by="month_day")

# include serology data

serology <- tibble(state_id=c("Washington",
                              "New York",
                              "Florida",
                              "Missouri",
                              "Utah",
                              "Connecticut"),
                   inf_pr=c(51737.16/7535591,
                                 724589.8/19542209,
                                 164046.9/21299325,
                                 161900/6109434,
                                 47400/2174312,
                                 176700/3576923),
                   case_pr=c(4606/7535591,
                                 60740/19542209,
                                 14672/21299325,
                                 6800/6109434,
                                 4500/2174312,
                                 29300/3576923),
                   survey_size=c(3265,
                                 2482,
                                 1742,
                                 1882,
                                 1132,
                                 1431),
                   inflation=inf_pr/case_pr,
                   date_begin=ymd(c("2020-03-23",
                                "2020-03-23",
                                "2020-04-06",
                                "2020-04-20",
                                "2020-04-20",
                                "2020-04-26")),
                   date_end=ymd(c("2020-04-01",
                              "2020-04-01",
                              "2020-04-10",
                              "2020-04-26",
                              "2020-05-03",
                              "2020-05-03")),
                   pop_size=c(4274336,
                                 9261183,
                                 6345945,
                                 6109434,
                                 2174312,
                                 3576923))

serology <- left_join(serology,mutate(select(ungroup(combined),month_day,state),key=1:n()),
                      by=c("date_end"="month_day","state_id"="state")) %>% 
  mutate(key=key - 1:n())

# remove rows from combined that are in the sero data
combined$key <- 1:nrow(combined)
combined_sero <- filter(combined, key %in% serology$key)

  #filter(state %in% c("New York","California","Alabama","Florida","Vermont","New Mexico"))

# look at how days after lockdown versus days after emergency compare

# combined %>% 
#   ungroup %>% 
#   filter(month_day==max(month_day)) %>% 
#   distinct(cases,state,lockdown_outbreak,emer_outbreak,state_pop) %>% 
#   ggplot(aes(y=lockdown_outbreak,
#              x=emer_outbreak)) +
#   geom_point(aes(size=cases/state_pop),colour="red",alpha=0.5) +
#   geom_text_repel(aes(label=state)) +
#   theme(panel.grid = element_blank(),
#         panel.background = element_blank()) + 
#   xlab("How Many Days Before the First COVID Case Was a State of Emergency Declared?") +
#   ylab("How Many Days Before the First COVID Case Was a Stay at Home Order Imposed?") +
#   ggtitle("Comparison of U.S. State Responses to First COVID-19 Case",
#           subtitle="Negative Numbers Indicate Policy Was Implemented After First COVID-19 Case")
# 
# ggsave("check_scatter.png",width=8,height=6)


saveRDS(combined,"combined.rds")

# create case dataset

cases_matrix <- select(combined,Difference,month_day,state) %>% 
  group_by(state,month_day) %>% 
  summarize(Difference=as.integer(mean(Difference)))

saveRDS(cases_matrix,"cases_matrix.rds")

cases_matrix_num <- as.matrix(select(ungroup(cases_matrix),-state,-month_day))

# create tests dataset

tests_matrix <- select(combined,tests,month_day,state) %>% 
  group_by(state,month_day) %>% 
  summarize(tests=as.integer(mean(tests)))

tests_matrix_num <- as.matrix(select(ungroup(tests_matrix),-state,-month_day))

just_data <- select(ungroup(combined),month_day,state,state_pop,trump,air="Air Pollution",
                      heart="Cardiovascular Deaths",
                      providers="Dedicated Health Care Provider",
                      young="Population under 18 years",
                      smoking="Smoking",
                      gdp,
                      masks="Health Resources",
                      testing_cap="Health Testing",
                      quarantine="Quarantine",
                      business="Restriction and Regulation of Businesses",
                    govt_services="Restriction and Regulation of Government Services",
                    mass_gathering="Restrictions of Mass Gatherings",
                    social_distance="Social Distancing",
                    sum_prot,
                    mask_wear,
                      trendline_approve,
                    trendline_extremely_concerned,
                    trendline_gotten_worse,
                    outbreak_time,
                      public_health="Public Health Funding",
                      prop_foreign) %>% arrange(state,month_day) %>% 
  mutate(outbreak_time=scale(outbreak_time),
         int_lockdown=quarantine*outbreak_time,
         int_business=business*outbreak_time,
         int_social=social_distance*outbreak_time,
         int_govt=govt_services*outbreak_time,
         int_gather=mass_gathering*outbreak_time)

covs <- select(ungroup(just_data),-state,-state_pop,-month_day,-outbreak_time,
               -int_lockdown,
               -int_business,
               -int_social,
               -int_gather,
               -int_govt,
               -business,
               -quarantine,
               -govt_services,
               -mass_gathering,
               -social_distance) %>% as.matrix

mobility <- select(ungroup(combined),month_day,state,retail:residential) %>% arrange(state,month_day)

covs_mob <- select(ungroup(mobility),-state,-month_day) %>% as.matrix

lockdown <- select(ungroup(just_data),state,month_day,quarantine,business,
                   mass_gathering,social_distance,govt_services,
                   int_lockdown,
                   int_business,
                   int_social,
                   int_gather,
                   int_govt) %>% arrange(state,month_day)

covs_lock <- select(ungroup(lockdown),-state,-month_day)

# now make sero data 

cases_sero <- cases_matrix_num[serology$key,]
tests_sero <- tests_matrix_num[serology$key,]
covs_sero <- covs[serology$key,]
covs_mob_sero <- covs_mob[serology$key,]
covs_lock_sero <- slice(covs_lock,serology$key)
world_infect_sero <- combined$world_infect[serology$key]


# now give to Stan

time_outbreak <- poly(scale(combined$outbreak_time),3)

time_global <- group_by(combined,month_day) %>% 
  summarize(time_global=sum(outbreak_time>1))

time_outbreak_sero <- time_outbreak[serology$key,]

# now filter out all the serology data

cases_matrix_num <- cases_matrix_num[-serology$key,]
tests_matrix_num <- tests_matrix_num[-serology$key,]
time_outbreak <- time_outbreak[-serology$key,]
world_infect <- combined$world_infect[-serology$key]

state_pop_sero <- combined$state_pop[serology$key]

state_pop <- combined$state_pop[-serology$key]
# cut effective sample size by half

#serology <- mutate(serology,survey_size=floor(survey_size/2))

# convert to numbers from dates/factorsß

real_data <- list(time_all=length(unique(combined$month_day)),
                 num_country=length(unique(combined$state)),
                 num_rows=nrow(combined) - nrow(serology),
                 num_rows_orig=rows_num_orig,
                 cc=as.numeric(factor(combined$state[-serology$key],levels=unique(combined$state))),
                 cc_sero=as.numeric(factor(serology$state_id,levels=unique(combined$state))),
                 R=nrow(serology), # number of CDC samples
                 S=ncol(covs),
                 G=ncol(covs_mob),
                 L=ncol(covs_lock),
                 country_id=as.numeric(factor(combined$state)),
                 date_id=as.numeric(factor(combined$date)),
                 sero=as.matrix(select(serology,-state_id,-date_end,-key,-date_begin)),
                 sero_row=serology$key,
                 country_pop=floor(state_pop),
                 country_pop_sero=floor(state_pop_sero),
                 cases=cases_matrix_num,
                 cases_sero=cases_sero,
                 phi_scale=.01,
                 test_max=combined$test_max,
                 count_outbreak=time_outbreak,
                 count_outbreak_sero=time_outbreak_sero,
                 tests=tests_matrix_num,
                 tests_sero=tests_sero,
                 month_cases=world_infect,
                 month_cases_sero=world_infect_sero,
                 suppress=covs,
                 suppress_sero=covs_sero,
                 mobility=covs_mob,
                 mobility_sero=covs_mob_sero,
                 lockdown=covs_lock,
                 lockdown_sero=covs_lock_sero,
                 sero_int=cbind(as.integer(floor(serology$inf_pr*serology$survey_size)),
                                        as.integer(serology$survey_size)))

saveRDS(real_data,"real_data.rds")

init_vals <- function() {
  list(phi_raw=c(100,100),
       world_infect=0.1,
       finding=0.5,
       suppress_effect_raw=rep(0,real_data$S),
       lockdown_effect_raw=rep(0,real_data$L),
       mob_effect_raw=rep(0,real_data$G),
       country_test_raw=rep(.1,real_data$num_country),
       pcr_spec=-5,
       alpha=c(-4,-4,-4))
}


```


```{r run_over_id_model_scale,include=F}


if(run_model) {
  pan_model_scale <- cmdstan_model("corona_tscs_betab_parallel.stan",
                                   cpp_options = list(stan_threads = TRUE))
  
  us_fit_scale_mod <- pan_model_scale$sample(data=real_data,chains=2,
                           iter_warmup=400,iter_sampling=200,
                           threads_per_chain = 8,
                           init=init_vals,
                           output_dir=".", validate_csv = FALSE,
                           parallel_chains=2)
  
  us_fit_scale <- rstan::sflist2stanfit(lapply(us_fit_scale_mod$output_files(),
                                          rstan::read_stan_csv))
  
  saveRDS(us_fit_scale,"../data/us_fit_scale.rds")
} else {
  #us_fit_scale <- readRDS("../data/us_fit_scale.rds")
  us_fit_scale <- readRDS("working_mod.rds")
}


```


```{r infectscaled,fig.cap="Approximate Total Number of COVID-19 Infected Individuals in the U.S. as of April 20th",fig.height=5}


combined <- mutate(ungroup(combined),key=1:n()) %>% 
  group_by(state) %>% 
  arrange(state,month_day) %>% 
  mutate(cum_sum_cases=Difference,
         recovered=coalesce(recovered,0),
         deaths=coalesce(deaths,0),
         cum_sum_cases = cum_sum_cases - deaths - recovered,
         lag_case=ifelse(dplyr::lag(cum_sum_cases,n=14)<0 & !is.na(dplyr::lag(cum_sum_cases,n=19)),0,
                         coalesce(dplyr::lag(cum_sum_cases,n=19),0)),
         cum_sum_cases = cum_sum_cases - lag_case)

# need to calculate estimates by hand

alpha <- as.matrix(us_fit_scale,"alpha")
sigma_poly <- as.matrix(us_fit_scale,"sigma_poly")
mu_poly <- as.matrix(us_fit_scale,"mu_poly")
poly1 <- as.matrix(us_fit_scale,"poly1")
poly2 <- as.matrix(us_fit_scale,"poly2")
poly3 <- as.matrix(us_fit_scale,"poly3")
world_infect <- as.matrix(us_fit_scale,"world_infect")
suppress_effect_raw <- as.matrix(us_fit_scale,"suppress_effect_raw")
lockdown_effect_raw <- as.matrix(us_fit_scale,"lockdown_effect_raw")
mob_effect_raw <- as.matrix(us_fit_scale,"mob_effect_raw")

# need to make non-centered polys

non1 <- mu_poly[,1] + sigma_poly[,1] * poly1
non2 <- mu_poly[,2] + sigma_poly[,2] * poly1
non3 <- mu_poly[,3] + sigma_poly[,3] * poly1

# calculate poly time trends

poly_time <- sapply(unique(real_data$cc), function(c) {
  real_data$count_outbreak[real_data$cc==c,1,drop=F] %*% t(non1[,c,drop=F])  +
   real_data$count_outbreak[real_data$cc==c,2,drop=F] %*% t(non2[,c,drop=F]) +
    real_data$count_outbreak[real_data$cc==c,3,drop=F] %*% t(non3[,c,drop=F])
}) %>% do.call(rbind,.)

prop_infected <-  alpha[,2] + 
          poly_time +
          t(world_infect %*% real_data$month_cases) +
          real_data$suppress %*% t(suppress_effect_raw) +
          as.matrix(real_data$lockdown) %*% t(lockdown_effect_raw) +
          real_data$mobility %*% t(mob_effect_raw);

prop_infected <- as_tibble(prop_infected) %>% 
  mutate(key=1:n()) %>% 
  gather(key="iter",value="estimate",-key)


all_est_state <- select(combined,deaths,recovered,month_day,state,key,cum_sum_cases,state_pop) %>% 
  left_join(prop_infected,by="key")

# merge in deaths/recovered

us_case_count <- group_by(combined,month_day) %>% 
  summarize(all_cum_sum=sum(cum_sum_cases),
            all_rec=sum(recovered),
            all_death=sum(deaths))

all_est_state <- left_join(all_est_state,us_case_count,by="month_day")

calc_sum <- all_est_state %>% 
  ungroup %>% 
  mutate(estimate=(plogis(estimate))*(state_pop)) %>% 
  group_by(state,iter) %>% 
  arrange(state,month_day) %>% 
  mutate(cum_est=estimate) %>% 
  group_by(month_day,iter,all_cum_sum,all_rec,all_death) %>%
  summarize(us_total=sum(cum_est)) %>%
  group_by(iter) %>%
  arrange(iter,month_day) %>%
  #mutate(us_total=us_total - coalesce(dplyr::lag(us_total,n=19),0)) %>%
  group_by(month_day,all_cum_sum) %>% 
  summarize(med_est=quantile(us_total,.5),
            high_est=quantile(us_total,.95),
            low_est=quantile(us_total,.05)) 

max_est <- as.integer(round(calc_sum$med_est[calc_sum$month_day==max(calc_sum$month_day)]))
high_max_est <- as.integer(round(calc_sum$high_est[calc_sum$month_day==max(calc_sum$month_day)]))
low_max_est <- as.integer(round(calc_sum$low_est[calc_sum$month_day==max(calc_sum$month_day)]))
max_obs <- calc_sum$all_cum_sum[calc_sum$month_day==max(calc_sum$month_day)]

options(scipen=999)

# load expert survey results

expert_survey <- read_csv("../data/consensusForecastsDB.csv") %>% 
  filter(questionLabel %in% c("QF5","QF4","QF3"),
         surveyIssued>ymd("2020-03-16")) %>% 
    mutate(keep=case_when(surveyIssued==ymd("2020-03-02")~"QF4",
                        surveyIssued==ymd("2020-03-09")~"QF6",
                        surveyIssued==ymd("2020-03-16")~"QF4",
                        surveyIssued==ymd("2020-03-23")~"QF4",
                        surveyIssued==ymd("2020-03-30")~"QF3",
                        TRUE~"reject")) %>% 
  filter(questionLabel==keep,cumprob>0.05,cumprob<0.95) %>% 
  group_by(surveyIssued,questionLabel) %>% 
  summarize(med_est=bin[abs(cumprob-0.5)==min(abs(cumprob-0.5))],
            low_est=bin[abs(cumprob-0.1)==min(abs(cumprob-.1))],
            high_est=bin[abs(cumprob-0.9)==min(abs(cumprob-.9))]) %>% 
  rename(month_day="surveyIssued")


# need to add in cumulative case counts

calc_sum %>% 
  ggplot(aes(y=med_est,x=month_day)) +
  geom_ribbon(aes(ymin=low_est,
  ymax=high_est),
  fill="blue",
  alpha=0.5) +
  geom_line(aes(y=all_cum_sum)) +
  theme_minimal() +
  ylab("Total Number Infected/Reported") +
  scale_y_continuous(labels=scales::comma) +
  labs(caption="Blue 5% - 95% HPD intervals show estimated infected and the black line\nshows observed cases from the New York Times.\nThese estimates are based on the assumption that as few as 10% of cases\nmay be reported based on SIR/SEIR models.") +
  annotate("text",x=ymd(c("2020-04-17","2020-04-17")),
           y=c(max_est-700000,max_obs+300000),
           hjust=1,
           vjust=0,
           fontface="bold",
           size=3,
           label=c(paste0("Estimated Infected:\n",formatC(low_max_est,big.mark=",",format = "f",digits=0)," - ",
                                                         formatC(high_max_est,big.mark=",",format = "f",digits=0)),
                   paste0("Total Reported Cases:\n",formatC(max_obs,big.mark=",")))) +
  annotate("text",x=ymd(c("2020-03-01","2020-03-12",
                          as.character(expert_survey$month_day))),
           y=c(300000,180000,expert_survey$high_est*1.01),
           vjust=0,
           fontface="bold",
           size=2,
           label=c("Li et al. March 8th",
                   "Perkins et al. March 26th",
                   paste0("Expert Survey\n",expert_survey$month_day)),alpha=0.8) +
  # previously published annotations
  annotate("pointrange",x=ymd("2020-03-01"),y=9001,ymin=2299,ymax=20403,alpha=0.5) +
  annotate("pointrange",x=ymd("2020-03-12"),y=22876,ymin=7451,ymax=53044,alpha=0.5) +
  geom_pointrange(data=expert_survey,aes(ymin=low_est,ymax=high_est),alpha=0.5) +
  xlab("Days Since Outbreak Start") +
  theme(panel.grid = element_blank(),
        legend.position = "top")

ggsave("est_vs_obs_experts.png")

```

By comparison, Figures \@ref(fig:infectscaled) and \@ref(fig:stateplot) show fully-identified models incorporating informative prior information suggesting that the ratio of tests to infected ratio individuals is probably no less than 10% of those infected (though it could very high). This information, as previously mentioned, was derived from simulation and statistical modeling of COVID-19 outbreaks so far suggesting that a large proportion of infected individuals are undetected [@li2020;@peak2020]. Based on this information, the scale of the latent infection process shown in Figure \@ref(fig:infectstate) can be further identified. In this figure, we calculate cumulative counts, and then subtract away reported recovered cases, deaths and a 14-day lag for recoveries from unreported infections (assuming such infections are mild). 

Figure \@ref(fig:infectscaled) shows that the likely present number of infected cases, excluding those who have recovered or died, is likely approaching 4 million presently infected individuals in the United States, in line with SIR/SEIR and expert survey projections released recently.^[See https://www.nytimes.com/2020/04/01/world/coronavirus-news.html?action=click&module=Spotlight&pgtype=Homepage for a recent overview.]  Furthermore, Figure \@ref(fig:stateplot) shows significant state by state heterogeneity, with New York showing the greatest number of infected, followed by California. There is some suggestive evidence that California's infected count growth may be slowing, though the large uncertainty interval suggests that this inference would be unwise without more data or assumptions.

As described in the modeling section, the model does not provide estimates that can be extrapolated into the future. Because the model is estimating empirical infection rates, it is primarily useful for adjusting empirical data, or the count of tests and cases and other background factors. However, because the model is substantively different than the SIR/SEIR simulations used to guide policy choices, it provides helpful external validation of these models incorporating observed information with minimal assumptions.

```{r stateplot,fig.cap="Average Cumulative Count of Infected People by U.S. State as of April 20th",fig.height=5,echo=F}
require(ggrepel)

# need different figures for case-level deaths + recovere

calc_sum_state <- all_est_state %>% 
  ungroup %>% 
  mutate(estimate=(plogis(estimate))*state_pop) %>% 
  #mutate(estimate=(plogis(estimate))*((state_pop))) %>%
  group_by(state,iter) %>% 
  arrange(state,iter,month_day) %>% 
  mutate(cum_est=estimate - recovered - deaths) %>% 
  #cum_est=cum_est-coalesce(dplyr::lag(cum_est,n=19),0)) %>% 
  #cum_sum_cases = cum_sum_cases - coalesce(dplyr::lag(cum_sum_cases,n=14),0)) %>% 
  group_by(month_day,state,cum_sum_cases) %>% 
  summarize(med_est=quantile(cum_est,.5),
            high_est=quantile(cum_est,.95),
            low_est=quantile(cum_est,.05)) 

saveRDS(calc_sum_state,"calc_sum_state.rds")

# Annotations

# get top 5 plus random 5 

top_5 <- filter(calc_sum_state,month_day==max(calc_sum_state$month_day)) %>% 
  arrange(desc(med_est)) %>% 
  ungroup %>% 
  slice(c(1:5,sample(6:length(unique(calc_sum_state$state)),5))) %>% 
  distinct %>% 
  mutate(label=paste0(state,":",formatC(low_est,big.mark=",",format = "f",digits=0)," - ",
                                                         formatC(high_est,big.mark=",",format = "f",digits=0)))

all <- calc_sum_state %>% 
  ggplot(aes(y=med_est,x=month_day)) +
  geom_line(aes(group=state,colour=med_est)) +
  # geom_ribbon(aes(ymin=low_est,
  # ymax=high_est,
  # group=state_num,
  # fill=suppress_measures),alpha=0.5) +
  theme_minimal() +
  scale_color_distiller(palette="Reds",direction=1) +
  ylab("Current Number Infected") +
  geom_text_repel(data=top_5,aes(x=month_day,y=med_est,label=label),
                  size=3,fontface="bold",segment.colour = NA) +
  scale_y_continuous(labels=scales::comma) +
  xlab("Days Since Outbreak Start") + 
  guides(colour="none") +
  theme(panel.grid = element_blank(),
        legend.position = "top")

# same calculations, but per capita

calc_sum_state <- all_est_state %>% 
  ungroup %>% 
  mutate(estimate=(plogis(estimate))*state_pop) %>% 
  group_by(state,iter) %>% 
  arrange(state,month_day) %>% 
  #mutate(cum_est=estimate - recovered - deaths,
         #cum_est=cum_est-coalesce(dplyr::lag(cum_est,n=19),0),
         mutate(cum_est=estimate/state_pop,
         cum_est=ifelse(cum_est<0,0,cum_est)) %>% 
  group_by(month_day,state,cum_sum_cases) %>% 
  summarize(med_est=quantile(cum_est,.5),
            high_est=quantile(cum_est,.95),
            low_est=quantile(cum_est,.05)) 

saveRDS(calc_sum_state,"percap.rds")

# Annotations

# get top 5 plus random 5 

top_5 <- filter(calc_sum_state,month_day==max(calc_sum_state$month_day)) %>% 
  ungroup %>% 
  arrange(desc(med_est)) %>% 
  ungroup %>% 
  slice(c(1:5,sample(6:length(unique(calc_sum_state$state)),5))) %>% 
  distinct %>% 
  mutate(label=paste0(state,":",formatC(low_est*100,big.mark=",",format = "f",digits=1)," - ",
                                                         formatC(high_est*100,big.mark=",",format = "f",digits=1)))


per_cap <- calc_sum_state %>% 
  ggplot(aes(y=med_est,x=month_day)) +
  geom_line(aes(group=state,colour=med_est)) +
  # geom_ribbon(aes(ymin=low_est,
  # ymax=high_est,
  # group=state_num,
  # fill=suppress_measures),alpha=0.5) +
  theme_minimal() +
  scale_color_distiller(palette="Reds",direction=1) +
  ylab("Percent Population Infected") +
  labs(caption="Some lines are labeled with uncertainty of estimates (5% - 95% Interval).\nThese estimates are based on the assumption that as few as\n10% of cases may be reported based on SIR/SEIR models. Does not exclude people\nwho may have recovered or died.") +
  geom_text_repel(data=top_5,aes(x=month_day,y=med_est,label=label),
                  size=3,fontface="bold",segment.colour = NA) +
  scale_y_continuous(labels=scales::percent) +
  xlab("Days Since Outbreak Start") + 
  guides(colour="none") +
  theme(panel.grid = element_blank(),
        legend.position = "top") 

all / per_cap

ggsave("certain_state_rates.png")
```

Figure \@ref(fig:rankcountries2) shows the results of the mediation analysis, in which we examined the indirect effect of lockdowns on each type of Google mobility data, permitting us to also calculate the controlled direct effect of lockdowns. The controlled direct effect is shown in panel A in Figure \@ref(fig:rankcountries2), while the mediated indirect effects are in panel B. The x axis of the plot shows the varying effect of lockdowns by the number of days following the first COVID-19 case in a given state that the lockdown was imposed. Generally speaking, the lockdowns imposed many days after the first COVID-19 case were in states like California and New York that saw cases relatively early, but reacted much later. 

What is clear from examining panel A is that the direct effect of lockdowns, excluding the effect mediated through mobility, is highly negative for states that adopted lockdowns many days after the first COVID-19 case, and quite the opposite for states adopting lockdowns very soon after their first COVID-19 case. In essence, lockdowns appear to be associated with more infections in these states, and do not show as great suppression as in the states that already had significant COVID-19 infections at time of first adoption.

The indirect effects in panel B may provide some insight as to why this is the case. While the indirect effects on grocery stores and parks are close to zero, the effects of the lockdown through retail and workplace are strongly negative. Furthermore, these effects are relatively constant regardless of when the lockdown was imposed. In other words, the increasing direct effect of lockdowns may be because it is having effects apart from the mobility. To the extent that lockdowns keep people away from retail establishments and workplaces, they have a strong supppression effect on the virus.

It is important to note, however, that lockdowns are associated with increases in infections when mediated through transit and residential mobility. Residential mobility is easier to explain as it is well-known that trapping people in homes will increase transmission within the home. Transit mobility, though, is harder to explain, and more research will need to be done to uncover the reasons for this association.

```{r rankcountries2,fig.cap="Mediated Effects of Lockdowns on Google Mobility Data",fig.height=4}

# to calculate these effects, need to marginalize over the 
# sample data, and also the number of mediators/direct effects

mob_effect <- rstan::extract(us_fit_scale,"mob_effect")[[1]]
lock_effect <- rstan::extract(us_fit_scale,"suppress_hier_const")[[1]]
direct_effect <- rstan::extract(us_fit_scale,"lockdown_effect")[[1]]

p_infected <- select(all_est_state,iter,state,month_day,estimate) %>% 
  spread(key="iter",value="estimate") %>% 
  ungroup %>% 
  select(-state,-month_day) %>% 
  as.matrix %>% 
  dlogis

# make a matrix of derivatives

if(run_model) {
  
    num_infected_deriv <- mutate(all_est_state,estimate=dlogis(estimate))
    
    # all the covariates we are looking at
    
    lock_covs <- c("quarantine","business","mass_gathering","social_distance","govt_services")
    int_locks <- c("int_lockdown","int_business","int_social","int_gather","int_govt")
    
    colnames(direct_effect) <- c(lock_covs,int_locks)
    
    dimnames(lock_effect)[[3]] <- c(lock_covs,int_locks)
    
    over_med <- lapply(1:ncol(covs_mob), function(m) {
      
      over_cov <- purrr::map2(lock_covs, int_locks,function(l,li) {

        over_mod <- lapply(seq(min(just_data[[li]]),max(just_data[[li]]),
                             length.out=10), function(d) {

          tibble(direct_eff=rowMeans((direct_effect[,l] + direct_effect[,li]*d)*p_infected),
                  indirect_eff=rowMeans((mob_effect[,m]*(lock_effect[,m,l] + lock_effect[,m,li]*d)*p_infected)),
                 total_eff=direct_eff + indirect_eff,
                 prop_med=indirect_eff/total_eff) %>% 
            mutate(mobility=colnames(covs_mob)[m],
                   lockdown=l,
                   day=d)
          
        }) %>% bind_rows
        
      }) %>% bind_rows

      
    }) %>% bind_rows
    
    saveRDS(over_med,"../data/mediator_eff.rds")
    
} else {
  over_med <- readRDS("../data/mediator_eff.rds")
}

over_med_agg <- over_med %>% 
  group_by(mobility,lockdown,day) %>% 
  summarize(med_direct=mean(direct_eff),
            high_direct=quantile(direct_eff,.95),
            low_direct=quantile(direct_eff,.05),
            med_indirect=mean(indirect_eff),
            high_indirect=quantile(indirect_eff,.95),
            low_indirect=quantile(indirect_eff,.05),
            med_total=mean(total_eff),
            high_total=quantile(total_eff,.95),
            low_total=quantile(total_eff,.05)) %>% 
  ungroup %>% 
  mutate(mobility=recode(mobility,grocery="Grocery",
                         parks="Parks",
                         residential="Residential",
                         transit="Transit",
                         retail="Retail",
                         workplaces="Workplaces",
                         residential="Residential")) %>% 
  gather(key="type",value="estimate",-mobility,-day,-lockdown) %>% 
  separate(type,into=c("est_type","variable")) %>% 
  spread(key="est_type",value="estimate")

direct <- over_med_agg %>% 
  filter(variable=="direct",mobility=="Residential") %>% 
  ggplot(aes(y=med,x=day*-1)) +
  geom_ribbon(aes(ymin=low,ymax=high),fill="blue",alpha=0.5) +
  geom_line(linetype=2) +
  geom_hline(yintercept = 0) +
  ylab("Marginal % Change in Infected Rate") +
  xlab("Days After First COVID-19 Case Until Lockdown Imposed") +
  scale_y_continuous(labels=scales::percent) +
  facet_wrap(~lockdown)

indirect <- over_med_agg %>% 
  filter(variable=="indirect") %>% 
  ggplot(aes(y=med,x=day*-1)) +
  geom_ribbon(aes(ymin=low,ymax=high),fill="blue",alpha=0.5) +
  geom_line(linetype=2) +
  geom_hline(yintercept = 0) +
    facet_wrap(~mobility+lockdown) +
  xlab("") +
  ylab("") +
  scale_y_continuous(labels=scales::percent)

total <- over_med_agg %>% 
  filter(variable=="total") %>% 
  ggplot(aes(y=med,x=day*-1)) +
  geom_ribbon(aes(ymin=low,ymax=high),fill="blue",alpha=0.5) +
  geom_line(linetype=2) +
  geom_hline(yintercept = 0) +
  facet_wrap(~mobility+lockdown) + 
scale_y_continuous(labels=scales::percent)

direct + indirect  +
      plot_annotation(tag_levels = "A",
                      caption="Results from mediation analysis using MCMC with Stan.\nPanel A shows controlled direct effect of lockdowns by number of days after first COVID-19 case lockdown imposed.\nPanel B shows indirect effect of lockdowns mediated through mobility data by number of days after first COVID-19 case lockdown imposed.") & theme(strip.placement = NULL)


```

Figure \@ref(fig:suppress1) shows the marginal effect of covariates in the model on the latent infection rate, expressed as the marginal increase in proportions for a standard deviation increase in the covariate.States with larger young and foreign-born populations, a higher GPD per capita and more health care providers are seeing increasingly higher infection rates. By contrast, states with more smokers and  better air quality are seeing increasingly fewer infections. These set of associations are not necessarily causal, as they are influenced by the spatial spread of the disease thus far, with outbreaks starting in wealthy coastal states and progressively moving inland. As the disease continues, we expect these associations to shift as we learn more about the effect of suppression policies targeting the disease.

However, it is important to note that state-level Trump vote share is associated with increasing COVID-19 infections, following public opinion showing wide partisan disagreement in concern over the virus.^[See https://www.vox.com/2020/3/15/21180506/coronavirus-poll-democrats-republicans-trump.] The marginal effect of 0.0004% is for a one-standard deviation increase in Trump vote share, or approximately ten percentage points. In a state as large as Texas (29 million people), such an increase in vote share would be associated with an approximate 116,000 increase in infections. Assuming a low infected fatality rate of 0.01, as some studies have suggested, that increase in Trump vote share would be associated with an additional 1160 deaths in the state of Texas.

```{r suppress1,echo=F,fig.height=7,fig.cap="Marginal Effects of Covariates on Latent Infection Rates for U.S. States"}

suppress_effect <- as.data.frame(us_fit_scale,"suppress_effect") %>% 
  mutate(iter=1:n()) %>% 
  gather(key="parameter",value="estimate",-iter) %>% 
  mutate(variable=as.numeric(str_extract(parameter,"(?<=\\[)[1-9][0-9]?0?")))

if(run_model) {
  
    over_all2 <- lapply(1:max(suppress_effect$variable), function(s) {
      
        this_eff <- (suppress_effect$estimate[suppress_effect$variable==s]*p_infected) 
        this_eff <- colMeans(this_eff)
      tibble(estimate=this_eff,
             variable=s)
      
    }) %>% bind_rows
    
    saveRDS(over_all2,"../data/over_all2.rds")
    
} else {
  over_all2 <- readRDS("../data/over_all2.rds")
}


over_all_sum2 <- group_by(over_all2,variable) %>% 
  summarize(med_est=median(estimate),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>% 
  mutate(model="Fully\nIdentified")
  
  

# calculate marginal effects

over_all_sum2 <- left_join(over_all_sum2,tibble(variable=1:ncol(covs),
                                        label=colnames(covs))) %>% 
  mutate(label=recode(label,
                      air="PM 2.5",
                      providers="No.\nProviders",
                      gdp="GDP",
                      heart="Cardiovascular\nDeaths",
                      day_emergency="Date\nEmergency",
                      young="% Population\n<18",
                      smoking="% Smokers",
                      trump="Trump\nVote Share",
                      prop_foreign="% Foreign-Born",
                      public_health="Public Health\nFunding"))

p3 <- over_all_sum2 %>%
  ggplot(aes(y=med_est,x=reorder(label,med_est))) +
  geom_pointrange(aes(ymin=low_est,ymax=high_est),alpha=0.8) +
  theme_minimal() +
  xlab("")  +
  ylab("Effect on Proportion Infected") +
  geom_hline(yintercept=0,linetype=2) +
  ggtitle("Cumulative Effect on\nProportion Infected") +
  geom_text(aes(label=scales::percent(med_est)),vjust=-1) +
  theme(panel.grid=element_blank(),
        plot.title = element_text(size=10,hjust=0.5),
        strip.text=element_text(face="bold"),
        axis.text.y=element_text(size=8)) +
  scale_y_continuous(labels=scales::percent) +
  coord_flip() 

p3 +  plot_annotation(caption="Marginal effects calculated as a 1-standard deviation change in a covariate on the\nlatent infection rate. 5% - 95% high posterior density intervals derived from\n100 Markov Chain Monte Carlo posterior draws.")

ggsave("marginal_id.png")

```

While the use of informative priors is very helpful for obtaining estimates that can be mapped back to actual number of infected persons, as we show in the supplementary information, we can in fact identify covariate effects (or at least their signs) without information about the ratio of tests to infected persons. To demonstrate this, we calculate time-varying marginal effects on the latent infected rate from both the partially-identified and fully-identified models and show them in Figure \@ref(fig:compareids). While there are certainly effect size and uncertainty differences between the models, the estimates are quite similar, and the signs are always in the same direction. For these reasons, we believe this model to be useful even in the case where there is no useful or credible information about the ratio of testing to infected.

Finally, Figure \@ref(fig:compmarg) compares the underlying parameter estimates from the latent Bayesian model and a binomial model of the observed case counts with the same covariates as predictors (including time trends). As can be seen, the estimates of these models can wildly diverge, with the observed case count model showing far larger and implausibly precise associations between covariates and case counts. Of particular worry is when covariates are correlated with a state's ability or willingness to test for the virus.^[We note that testing may be conducted for the virus using PCR, or for the disease, which would also include serology. This also may explain why the observed model in Figure \@ref(fig:compmarg) shows such a high effect of GDP per capita on reducing infection counts.]


For example, the coefficient for public health shows an implausibly large positive association (+8 on the *logit* scale) in the observed cases model, suggesting that the more public health funding in a state, the higher the infection rate. The latent infection model, by contrast, shows a positive but very small effect. To show the potential confounding in this result, Figure \@ref(fig:comptests) plots public health funding against state COVID-19 tests per capita, revealing states with more infected people--Rhode Island and New York--have implemented many tests and are also have more public health funding. As such, the observed data model is likely obfuscating the strength of the correlation between public health funding and the number of tests with the spread of the disease.


```{r mobility,fig.cap="Effect of Google Mobility Data on COVID-19 Spread"}



```


In addition to the estimation of covariates, the model provides further useful information by parameterizing the relationship between the unobserved infection rate and the number of tests conducted in a given state. These individual parameters are shown in Figure \@ref(fig:tpi). The scale of the y axis shows by how much tests will increase as a percentage of the population for a one percent increase in infection rates in each state as of May 24th. The plot shows that some states could increase their tests as a proportion of the population by roughly double this infection rate (New York, Rhode Island), while other states like Texas and Montana are increasing tests at a much slower rate than the growth rate in infections, a worrying sign.


```{r tpi,fig.height=6,fig.cap="Measuring States' Testing Rates Relative to Infection Rates"}

# convert to marginal changes

test_var <- as.data.frame(us_fit_scale,"country_test_raw") %>%
  mutate(iter=1:n()) %>%
  gather(key="variable",value="estimate",-iter) %>%
  mutate(state_num=as.numeric(str_extract(variable,"(?<=\\[)[1-9][0-9]?0?")))

this_infect <- filter(ungroup(all_est_state),month_day==max(month_day)) %>% 
  mutate(estimate2=plogis(estimate)+.01)

this_test_max <- ungroup(combined) %>% 
  group_by(state) %>% 
  arrange(state,month_day) %>% 
  mutate(max_test=roll_max(tests)/state_pop) %>% 
  filter(month_day==max(month_day)) %>% 
  select(state,max_test)

test_var <- left_join(test_var,tibble(state_num=1:length(unique(combined$state)),
                                                state=unique(combined$state),
                                                state_pop=unique(combined$state_pop))) %>% 
  left_join(this_test_max,"state")

alpha_test <- as.data.frame(us_fit_scale,pars="alpha[1]")

test_max_par <- as.data.frame(us_fit_scale,"test_max_par")

# loop over infections

over_states <- lapply(unique(test_var$state), function(s) {
  tibble(estimate=plogis(alpha_test$`alpha[1]` + test_var$estimate[test_var$state==s] * this_infect$estimate2[this_infect$state==s] + test_max_par$test_max_par*test_var$max_test[test_var$state==s]) - plogis((alpha_test$`alpha[1]` + test_var$estimate[test_var$state==s] * this_infect$estimate[this_infect$state==s] +
                                                                                                                                                                                                                    test_max_par$test_max_par*test_var$max_test[test_var$state==s])),
         state=s)
}) %>% bind_rows

saveRDS(over_states,"test_data.rds")

over_states %>%
  group_by(state) %>%
    summarize(med_est=quantile(estimate,.5),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %>%
  ggplot(aes(y=med_est,x=reorder(state,med_est))) +
  geom_pointrange(aes(ymin=low_est,ymax=high_est)) +
  theme_minimal() +
  theme(panel.grid = element_blank()) +
  coord_flip() +
  geom_hline(yintercept=.01) +
  xlab("") +
  scale_y_continuous(labels=scales::percent) +
  # scale_y_continuous(breaks=c(.96,.98,1,1.02,1.04,1.06),
  #                    labels = c("Ahead of\nCurve",".98","Keeping\nUp","1.02","1.04","Behind\nCurve")) +
  # annotate("text",y=c(.96,1.05),x=c("Georgia","North Dakota"),label=c("Testing More\nThan Infection Growth","Testing Less\nThan Infection Growth"),size=3) +
  # geom_hline(yintercept=1,linetype=2) +
  labs(caption = "Scale of the estimates shows how increases in the infection rate increase\nthe number of tests in a given state. Higher numbers indicate fewer tests\nconducted for a unit increase in the infection rate (on the logit scale).") +
  ylab("Additional Proportion Tested of Population for Every Percent Increase in Infected")

ggsave("testing.png",scale=1.1)

```

We would note that this information is also helpful to policy makers and others trying to make sense of observed case counts given the limitation in testing thus far. Our estimates help take into account these known biases and adjust them based on differences between states and within states in terms of disease trajectories. We believe this model can be used to help understand disease trends and factors associated with it even in the relatively data-poor environment many countries find themselves in. 

# Conclusion

This model was devised to permit the identification of suppression measures and social, political and economic factors on the spread of COVID-19. It is not intended to be a replacement or alternative to the disease forecasting literature, especially as this model relies on SIR/SEIR estimates for full identification. If anything, this modeling exercise shows why explicit mechanistic epidemiological models are so important: without them it is literally impossible to know the total number of infected people on a given day. This model's simplicity and ability to use empirical data are its main features, and the hope is that it can be used and extended by researchers looking at government policies and other tertiary factors on the spread of the disease. At the very least, the model provides realistic uncertainty intervals taking into account very real biases in the observed data.

In addition, the model provides insight into how the number of tests undertaken by a given country or area compares to the probably number of infections. These parameter estimates can be used to understand whether a state's testing exceeds, is the same as or is less than the number of infected individuals. Given the wide problem of data scarcity in understanding the disease's spread, we hope this model can be used to make the most of empirical evidence. 

To fit the model, it is necessary to have at least an estimate of how many tests have been conducted. The  [CoronaNet](https://lumesserschmidt.github.io/CoronaNet/) project is currently working to obtain testing data, in addition to information about government policy responses to COVID-19, in an effort to better understand the role and success of variation in country policy responses to date. 

# Bibliography


